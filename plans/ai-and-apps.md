# AI & Apps

LLM inference, Home Assistant, custom apps, and Immich enhancements.

## Inference Improvements

- [ ] GPU passthrough for Ollama on Gaming PC (NVIDIA driver upgrade 300s → 470+, or native Windows Ollama install)
- [ ] Model optimization — investigate quantization options (Q4_K_M vs Q5_K_M), smaller capable models, offloading strategies
- [ ] Resolve Ollama permanent hosting (GPU upgrade, dedicated server, or cloud API — MacBook is temporary)

## Home Assistant

- [ ] Set up Home Assistant instance
- [ ] Enable MCP Server integration for LLM access
- [ ] Thermostat monitoring (ZWave connector)

## Custom Wrapper App

- [ ] Flutter app using mcp_client package
- [ ] Unified interface to all homelab services via MCP
- [ ] LLM integration for natural language commands

## Immich Enhancements

- [ ] Consider: Move Immich "brain" to NAS, only wake PC for transcoding
- [ ] Set up locally hosted map tiles (replace tiles.immich.cloud)
- [ ] LLM integration for batch metadata updates
